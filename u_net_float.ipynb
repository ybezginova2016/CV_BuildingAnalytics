{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Задачи сегментации фасадов и детекции объектов (окна, колонны)\n",
    "\n",
    "#### Необходимо обучить собственный алгоритм tensorflow сегментации фасада главного здания, а также алгоритм определения числа окон и числа колонн.\n",
    "#### Заказчик хотел бы выделять на фото фасад \"главного\" здания, если на изображении несколько зданий.\n",
    "\n",
    "Решение задачи требует создания нескольких алгоритмов машинного обучения для решения трех различных задач:\n",
    "\n",
    "* Сегментация фасада главного здания на изображении\n",
    "* Определение числа окон на фасаде\n",
    "* Определение числа колонн на фасаде\n",
    "\n",
    "Для решения каждой из этих задач можно использовать различные алгоритмы машинного обучения, такие как нейронные сети и алгоритмы компьютерного зрения.\n",
    "\n",
    "Например, для **сегментации фасада главного здания на изображении можно использовать алгоритмы семантической сегментации**, такие как FCN, U-Net или SegNet, которые могут обучаться на размеченных данных, где каждый пиксель на изображении помечен как принадлежащий к фасаду или нет.\n",
    "\n",
    "Для **определения числа окон и числа колонн на фасаде можно использовать алгоритмы детектирования объектов**, такие как Faster R-CNN, RetinaNet или YOLO, которые могут обучаться на размеченных данных, где каждый объект (окно или колонна) на изображении помечен с помощью ограничивающей рамки.\n",
    "\n",
    "Также можно использовать готовые модели машинного обучения, предварительно обученные на больших наборах данных, такие как ImageNet, и дообучать их на своих размеченных данных.\n",
    "\n",
    "Однако, чтобы создать алгоритмы машинного обучения для решения этих задач, необходимо иметь доступ к размеченным данным, то есть набору изображений, на которых фасады главного здания помечены, а также размечены окна и колонны. Такие данные были получены на первом этапе предобработки данных путем ручной разметки изображений.\n",
    "\n",
    "Кроме того, необходимо провести оценку качества полученных моделей машинного обучения на тестовых данных, чтобы убедиться в их эффективности и точности. Для этого можно использовать метрики, такие как точность, полноту и F1-меру, а также визуально сравнить результаты работы модели с оригинальными изображениями.\n",
    "\n",
    "Кроме того, стоит учитывать, что **задача определения числа окон и колонн на фасаде может быть достаточно сложной, особенно если на изображении находятся объекты разных размеров, расположенные на разном расстоянии от камеры, в разных условиях освещения и т.д. Поэтому для получения точных результатов может потребоваться использование дополнительных методов предобработки изображений, таких как поворот, масштабирование и улучшение контрастности.**\n",
    "\n",
    "*Для обучения моделей используем U-Net и YOLO, так как работаем без GPU-ускорителя и ограничены домашним ноутбуком и мощностями Google Colab.*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Сегментация фасадов с помощью U-Net\n",
    "\n",
    "**U-Net - это нейронная сеть для семантической сегментации изображений, разработанная для медицинских изображений, но также успешно применяемая для сегментации фасадов зданий.**\n",
    "\n",
    "Для проведения сегментации фасадов с помощью U-Net необходимо иметь размеченный набор данных, на котором обучится модель. Разметка данных представляет собой пометку каждого пикселя на изображении как принадлежащего к фасаду или не принадлежащего. Этот процесс может быть трудоемким и затратным.\n",
    "\n",
    "#### Обучение модели включает в себя:\n",
    "\n",
    "1. Нормализацию и предобработку данных, таких как изменение размера изображений, масштабирование значений пикселей и т.д.\n",
    "2. Разбиение данных на обучающую, валидационную и тестовую выборки.\n",
    "3. Определение архитектуры модели и настройка параметров, таких как количество слоев и фильтров, функция активации, оптимизатор и т.д.\n",
    "4. Обучение модели на обучающей выборке с использованием алгоритма обратного распространения ошибки и минимизации функции потерь.\n",
    "5. Оценка модели на валидационной выборке и настройка параметров модели для улучшения ее качества.\n",
    "6. Тестирование модели на тестовой выборке для оценки ее точности и эффективности.\n",
    "\n",
    "После того, как модель обучена, мы можем использовать ее для сегментации фасадов на новых изображениях. Для этого необходимо применить модель к каждому пикселю изображения и определить, принадлежит ли он фасаду или нет. Результаты сегментации могут быть сохранены в виде маски, где каждый пиксель помечен как принадлежащий фасаду или нет"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "        filename  background  facade  window  door  cornice  sill  balcony  \\\n0  cmp_b0001.png           0       1       1     1        1     1        1   \n1  cmp_b0002.png           1       0       1     1        1     1        1   \n2  cmp_b0003.png           1       1       0     1        1     1        1   \n3  cmp_b0004.png           1       1       1     0        1     1        1   \n4  cmp_b0005.png           1       1       1     1        0     1        1   \n\n   blind  deco  molding  pillar  shop  \n0      1     1        1       1     1  \n1      1     1        1       1     1  \n2      1     1        1       1     1  \n3      1     1        1       1     1  \n4      1     1        1       1     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>background</th>\n      <th>facade</th>\n      <th>window</th>\n      <th>door</th>\n      <th>cornice</th>\n      <th>sill</th>\n      <th>balcony</th>\n      <th>blind</th>\n      <th>deco</th>\n      <th>molding</th>\n      <th>pillar</th>\n      <th>shop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cmp_b0001.png</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cmp_b0002.png</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cmp_b0003.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cmp_b0004.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cmp_b0005.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_png = pd.read_csv(\"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\labels_png.csv\")\n",
    "label_png.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "        filename  facade\n0  cmp_b0001.png       1\n1  cmp_b0002.png       0\n2  cmp_b0003.png       1\n3  cmp_b0004.png       1\n4  cmp_b0005.png       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>facade</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cmp_b0001.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cmp_b0002.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cmp_b0003.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cmp_b0004.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cmp_b0005.png</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_facade = label_png[['filename', 'facade']].copy()\n",
    "labels_facade.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "labels_facade.to_csv('labels_facade.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "          filename  background  facade  window  door  cornice  sill  balcony  \\\n0    cmp_b0001.jpg           0       1       1     1        1     1        1   \n1    cmp_b0001.png           0       1       1     1        1     1        1   \n2    cmp_b0002.jpg           1       0       1     1        1     1        1   \n3    cmp_b0002.png           1       0       1     1        1     1        1   \n4    cmp_b0003.jpg           1       1       0     1        1     1        1   \n..             ...         ...     ...     ...   ...      ...   ...      ...   \n751  cmp_b0376.png           1       1       0     1        1     0        0   \n752  cmp_b0377.jpg           1       1       0     1        1     1        0   \n753  cmp_b0377.png           1       1       0     1        1     1        0   \n754  cmp_b0378.jpg           1       1       0     1        1     1        0   \n755  cmp_b0378.png           1       1       0     1        1     1        0   \n\n     blind  deco  molding  pillar  shop  \n0        1     1        1       1     1  \n1        1     1        1       1     1  \n2        1     1        1       1     1  \n3        1     1        1       1     1  \n4        1     1        1       1     1  \n..     ...   ...      ...     ...   ...  \n751      1     1        1       1     1  \n752      1     1        1       1     1  \n753      1     1        1       1     1  \n754      0     1        1       1     1  \n755      0     1        1       1     1  \n\n[756 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>background</th>\n      <th>facade</th>\n      <th>window</th>\n      <th>door</th>\n      <th>cornice</th>\n      <th>sill</th>\n      <th>balcony</th>\n      <th>blind</th>\n      <th>deco</th>\n      <th>molding</th>\n      <th>pillar</th>\n      <th>shop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cmp_b0001.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cmp_b0001.png</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cmp_b0002.jpg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cmp_b0002.png</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cmp_b0003.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>cmp_b0376.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>752</th>\n      <td>cmp_b0377.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>cmp_b0377.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>cmp_b0378.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>755</th>\n      <td>cmp_b0378.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>756 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the path to the directory containing the image data and the CSV file with labels\n",
    "data_dir = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\\"\n",
    "labels_file = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\labels.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(os.path.join(data_dir, labels_file))\n",
    "labels_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "train_size = 0.6  # Proportion of data to use for training\n",
    "val_size = 0.4  # Proportion of data to use for validation\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=12345)\n",
    "train_indices, val_indices = next(sss.split(X=labels_df['filename'], y=labels_df['facade']))\n",
    "\n",
    "train_df = labels_df.iloc[train_indices]\n",
    "val_df = labels_df.iloc[val_indices]\n",
    "\n",
    "# Create subdirectories for the subsets\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'valid')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Copy the image files to the subdirectories\n",
    "for _, row in train_df.iterrows():\n",
    "    src_path = os.path.join(data_dir, 'base', row['filename'])\n",
    "    dst_path = os.path.join(train_dir, row['filename'])\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "for _, row in val_df.iterrows():\n",
    "    src_path = os.path.join(data_dir, 'base', row['filename'])\n",
    "    dst_path = os.path.join(val_dir, row['filename'])\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Save the labels for the subsets to separate CSV files\n",
    "train_df.to_csv(os.path.join(data_dir, 'train_labels.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(data_dir, 'valid_labels.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# class_names = list(train_generator.class_indices.keys())\n",
    "# print('Названия классов из читаемой папки:', class_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "MAIN_DIR = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\\"\n",
    "\n",
    "TRAIN_DIR = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\train\"\n",
    "VALID_DIR = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\valid\"\n",
    "\n",
    "TRAIN_LABELS = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\train_labels.csv\"\n",
    "VALID_LABELS = \"C:\\\\Users\\\\HOME\\\\PycharmProjects\\\\CV_BuildingAnalytics\\\\valid_labels.csv\"\n",
    "\n",
    "INPUT_SHAPE = (128, 128, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def load_train(path):\n",
    "    labels = pd.read_csv(path + 'train_labels.csv')\n",
    "    labels['facade'] = labels['facade'].astype(str) # Convert facade column to string type\n",
    "    train_datagen = ImageDataGenerator(validation_split=0.25, horizontal_flip=True, rescale=1./255)\n",
    "    X_train = train_datagen.flow_from_dataframe(\n",
    "        dataframe=labels,\n",
    "        directory=path + '\\\\train',\n",
    "        x_col='filename',\n",
    "        y_col='facade',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        subset='training',\n",
    "        seed=12345)\n",
    "\n",
    "    # Modify the labels to have shape (batch_size, 128, 128, 1)\n",
    "    y_train = np.expand_dims(X_train.labels, axis=-1)\n",
    "    y_train = np.tile(y_train, (1, 128, 128, 1))\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def load_valid(path):\n",
    "    labels = pd.read_csv(path + 'valid_labels.csv')\n",
    "    labels['facade'] = labels['facade'].astype(str) # Convert facade column to string type\n",
    "    test_datagen = ImageDataGenerator(validation_split=0.25, rescale=1./255)\n",
    "    X_valid = test_datagen.flow_from_dataframe(\n",
    "        dataframe=labels,\n",
    "        directory=path + '\\\\valid',\n",
    "        x_col='filename',\n",
    "        y_col='facade',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        subset='validation',\n",
    "        seed=12345)\n",
    "\n",
    "    # Modify the labels to have shape (batch_size, 128, 128, 1)\n",
    "    y_valid = np.expand_dims(X_valid.labels, axis=-1)\n",
    "    y_valid = np.tile(y_valid, (1, 128, 128, 1))\n",
    "\n",
    "    return X_valid, y_valid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# def dice_coefficient_loss(y_true, y_pred):\n",
    "#     axis = (1, 2)\n",
    "#     numerator = 2.0 * tf.reduce_sum(y_true * y_pred, axis=axis)\n",
    "#     denominator = tf.reduce_sum(y_true + y_pred, axis=axis)\n",
    "#     return 1.0 - tf.reduce_mean(numerator / denominator)\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - (dice_coefficient(y_true, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# def create_model(input_shape):\n",
    "#\n",
    "#     inputs = tf.keras.layers.Input(input_shape)\n",
    "#     s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "#\n",
    "#     conv1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(s)\n",
    "#     conv1 = tf.keras.layers.Dropout(0.1)(conv1)\n",
    "#     conv1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv1)\n",
    "#     pool1 = tf.keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "#\n",
    "#     conv2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(pool1)\n",
    "#     conv2 = tf.keras.layers.Dropout(0.1)(conv2)\n",
    "#     conv2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv2)\n",
    "#     pool2 = tf.keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "#\n",
    "#     conv3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(pool2)\n",
    "#     conv3 = tf.keras.layers.Dropout(0.2)(conv3)\n",
    "#     conv3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv3)\n",
    "#     pool3 = tf.keras.layers.MaxPooling2D((2, 2))(conv3)\n",
    "#\n",
    "#     conv4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(pool3)\n",
    "#     conv4 = tf.keras.layers.Dropout(0.2)(conv4)\n",
    "#     conv4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv4)\n",
    "#     pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "#\n",
    "#     conv5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(pool4)\n",
    "#     conv5 = tf.keras.layers.Dropout(0.3)(conv5)\n",
    "#     conv5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv5)\n",
    "#\n",
    "#     upconv6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "#     upconv6 = tf.keras.layers.concatenate([upconv6, conv4])\n",
    "#     conv6 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(upconv6)\n",
    "#     conv6 = tf.keras.layers.Dropout(0.2)(conv6)\n",
    "#     conv6 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv6)\n",
    "#\n",
    "#     upconv7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "#     upconv7 = tf.keras.layers.concatenate([upconv7, conv3])\n",
    "#     conv7 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(upconv7)\n",
    "#     conv7 = tf.keras.layers.Dropout(0.2)(conv7)\n",
    "#     conv7 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv7)\n",
    "#\n",
    "#     upconv8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "#     upconv8 = tf.keras.layers.concatenate([upconv8, conv2])\n",
    "#     conv8 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(upconv8)\n",
    "#     conv8 = tf.keras.layers.Dropout(0.1)(conv8)\n",
    "#     conv8 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv8)\n",
    "#\n",
    "#     upconv9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "#     upconv9 = tf.keras.layers.concatenate([upconv9, conv1], axis=3)\n",
    "#     conv9 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(upconv9)\n",
    "#     conv9 = tf.keras.layers.Dropout(0.1)(conv9)\n",
    "#     conv9 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "#                                 padding='same')(conv9)\n",
    "#\n",
    "#     outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "#\n",
    "#     model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "#     optimizer = Adam(learning_rate=0.0001)\n",
    "#\n",
    "#     model.compile(optimizer, loss='binary_crossentropy', metrics=[dice_coefficient,'accuracy'])\n",
    "#\n",
    "#     return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def basic_unet(input_shape):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    # Encoder path\n",
    "    conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "\n",
    "    # Decoder path\n",
    "    up3 = tf.keras.layers.Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(conv2)\n",
    "    up3 = tf.keras.layers.concatenate([up3, conv1], axis=3)\n",
    "    conv3 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up3)\n",
    "    conv3 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(conv3)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer, loss='binary_crossentropy', metrics=[dice_coefficient,'accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                X_train, y_train,\n",
    "                X_valid, y_valid,\n",
    "                batch_size=None,\n",
    "                epochs=5,\n",
    "                steps_per_epoch=None,\n",
    "                validation_steps=None):\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = int(np.ceil(X_train.samples / batch_size))\n",
    "    if validation_steps is None:\n",
    "        validation_steps = int(np.ceil(X_valid.samples / batch_size))\n",
    "\n",
    "    train_generator = ImageDataGenerator().flow(X_train, y_train, batch_size=batch_size)\n",
    "    valid_generator = ImageDataGenerator().flow(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "    callbacks = [EarlyStopping(patience=6, monitor='loss')]\n",
    "\n",
    "    model.fit_generator(train_generator,\n",
    "                        validation_data=valid_generator,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_train(MAIN_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "X_valid, y_valid = load_valid(MAIN_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# u_net = create_model(INPUT_SHAPE)\n",
    "# u_net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"model_2\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=128>, <tf.Tensor: shape=(), dtype=int32, numpy=128>, <tf.Tensor: shape=(), dtype=int32, numpy=3>]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m basic_unet \u001B[38;5;241m=\u001B[39m \u001B[43mbasic_unet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_SHAPE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m basic_unet\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py:216\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[1;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInputs to a layer should be tensors. Got: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(input_spec):\n\u001B[1;32m--> 216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    217\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLayer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m expects \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(input_spec)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m input(s),\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but it received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(inputs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m input tensors. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInputs received: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    220\u001B[0m     )\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_index, (x, spec) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(inputs, input_spec)):\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mValueError\u001B[0m: Layer \"model_2\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=128>, <tf.Tensor: shape=(), dtype=int32, numpy=128>, <tf.Tensor: shape=(), dtype=int32, numpy=3>]"
     ]
    }
   ],
   "source": [
    "basic_u\n",
    "et = basic_unet(INPUT_SHAPE)\n",
    "basic_unet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# history = train_model(u_net, X_train, y_train, X_valid, y_train, batch_size=16, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.00 MiB for an array with shape (16, 128, 128, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history_basic \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbasic_unet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[42], line 14\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, X_train, y_train, X_valid, y_valid, batch_size, epochs, steps_per_epoch, validation_steps)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     12\u001B[0m     validation_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(np\u001B[38;5;241m.\u001B[39mceil(X_valid\u001B[38;5;241m.\u001B[39msamples \u001B[38;5;241m/\u001B[39m batch_size))\n\u001B[1;32m---> 14\u001B[0m train_generator \u001B[38;5;241m=\u001B[39m \u001B[43mImageDataGenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m valid_generator \u001B[38;5;241m=\u001B[39m ImageDataGenerator()\u001B[38;5;241m.\u001B[39mflow(X_valid, y_valid, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[0;32m     17\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m [EarlyStopping(patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:1545\u001B[0m, in \u001B[0;36mImageDataGenerator.flow\u001B[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001B[0m\n\u001B[0;32m   1486\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflow\u001B[39m(\n\u001B[0;32m   1487\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1488\u001B[0m     x,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1498\u001B[0m     subset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1499\u001B[0m ):\n\u001B[0;32m   1500\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001B[39;00m\n\u001B[0;32m   1501\u001B[0m \n\u001B[0;32m   1502\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1543\u001B[0m \n\u001B[0;32m   1544\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mNumpyArrayIterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1546\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1547\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1548\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1551\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1553\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1554\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_to_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_to_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1555\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1556\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_class_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_class_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1558\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:711\u001B[0m, in \u001B[0;36mNumpyArrayIterator.__init__\u001B[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001B[0m\n\u001B[0;32m    704\u001B[0m     x_misc \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    706\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(y):\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    708\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`x` (images tensor) and `y` (labels) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    709\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshould have the same length. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    710\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound: x.shape = \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, y.shape = \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 711\u001B[0m         \u001B[38;5;241m%\u001B[39m (\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape, np\u001B[38;5;241m.\u001B[39masarray(y)\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    712\u001B[0m     )\n\u001B[0;32m    713\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(sample_weight):\n\u001B[0;32m    714\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    715\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`x` (images tensor) and `sample_weight` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    716\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshould have the same length. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    717\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound: x.shape = \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, sample_weight.shape = \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    718\u001B[0m         \u001B[38;5;241m%\u001B[39m (np\u001B[38;5;241m.\u001B[39masarray(x)\u001B[38;5;241m.\u001B[39mshape, np\u001B[38;5;241m.\u001B[39masarray(sample_weight)\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    719\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:156\u001B[0m, in \u001B[0;36mIterator.__next__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:168\u001B[0m, in \u001B[0;36mIterator.next\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    165\u001B[0m     index_array \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_generator)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;66;03m# The transformation of images is not under thread lock\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;66;03m# so it can be done in parallel\u001B[39;00m\n\u001B[1;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_batches_of_transformed_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex_array\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:363\u001B[0m, in \u001B[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001B[1;34m(self, index_array)\u001B[0m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_batches_of_transformed_samples\u001B[39m(\u001B[38;5;28mself\u001B[39m, index_array):\n\u001B[0;32m    356\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Gets a batch of transformed samples.\u001B[39;00m\n\u001B[0;32m    357\u001B[0m \n\u001B[0;32m    358\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;124;03m        A batch of transformed samples.\u001B[39;00m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 363\u001B[0m     batch_x \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    364\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mindex_array\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\n\u001B[0;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    366\u001B[0m     \u001B[38;5;66;03m# build batch of image data\u001B[39;00m\n\u001B[0;32m    367\u001B[0m     \u001B[38;5;66;03m# self.filepaths is dynamic, is better to call it once outside the loop\u001B[39;00m\n\u001B[0;32m    368\u001B[0m     filepaths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepaths\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 3.00 MiB for an array with shape (16, 128, 128, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "history_basic = train_model(basic_unet, X_train, y_train, X_valid, y_train, batch_size=16, epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 2"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
